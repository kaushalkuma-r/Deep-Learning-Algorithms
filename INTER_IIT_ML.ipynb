{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INTER_IIT_ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushalkuma-r/Deep-Learning-Algorithms/blob/main/INTER_IIT_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "d0PYp7rnqiHW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_url(base_url , comp):\n",
        "    \n",
        "    url = base_url\n",
        "    \n",
        "    # add each component to the base url\n",
        "    for r in comp:\n",
        "        url = '{}/{}'.format(url, r)\n",
        "        \n",
        "    return url"
      ],
      "metadata": {
        "id": "bsxiUi0fxyFF"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_list=['2019','2020','2021','2018','2017','2016','2015','2014','2013','2012','2011','2010','2009','2008','2007','2006','2005','2004']\n",
        "hdr= {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'}"
      ],
      "metadata": {
        "id": "9KUB2gChQypb"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_url=[]\n",
        "def scraping_files(year1):\n",
        "  \n",
        "# define the urls needed to make the request, let's start with all the daily filings\n",
        "  base_url = r\"https://www.sec.gov/Archives/edgar/full-index\"\n",
        "\n",
        "\n",
        "  # The daily-index filings, requir/e a year and content type (html, json, or xml).\n",
        "  year_url = make_url(base_url, [year1, 'index.json'])\n",
        "\n",
        "# Display the new Year URL\n",
        "  # print('-'*100)\n",
        "  # print('Building the URL for Year: {}'.format(year))\n",
        "  print(\"URL Link: \" + year_url)\n",
        "\n",
        "# request the content for 2019, remember that a JSON strucutre will be sent back so we need to decode it.\n",
        "  content = requests.get(year_url,headers=hdr)\n",
        "  print(content)\n",
        "  decoded_content = content.json()\n",
        "\n",
        "# the structure is almost identical to other json requests we've made. Go to the item list.\n",
        "# AGAIN ONLY GRABBING A SUBSET OF THE FULL DATASET \n",
        "  for item in decoded_content['directory']['item'][0:1]:\n",
        "    \n",
        "    # get the name of the folder\n",
        "    print('-'*100)\n",
        "    print('Pulling url for Quarter: {}'.format(item['name']))\n",
        "    \n",
        "    # The daily-index filings, require a year, a quarter and a content type (html, json, or xml).\n",
        "    qtr_url = make_url(base_url, [year1, item['name'], 'index.json'])\n",
        "    \n",
        "    # print out the url.\n",
        "    print(\"URL Link: \" + qtr_url)\n",
        "    \n",
        "    # Request, the new url and again it will be a JSON structure.\n",
        "    file_content = requests.get(qtr_url,headers=hdr)\n",
        "    decoded_content = file_content.json()\n",
        "    \n",
        "    # print('-'*100)\n",
        "    # print('Pulling files')\n",
        "\n",
        "    # for each file in the directory items list, print the file type and file href.\n",
        "    # AGAIN DOING A SUBSET\n",
        "    for file in decoded_content['directory']['item'][0:20]:\n",
        "        \n",
        "        file_url = make_url(base_url, [year1, item['name'], file['name']])\n",
        "        list_url.append(file_url)\n",
        "        # print(\"File URL Link: \" + file_url)"
      ],
      "metadata": {
        "id": "9gc4sef6HPW9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(year_list)):\n",
        "  scraping_files(year_list[i])"
      ],
      "metadata": {
        "id": "QWehmjL62mf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "002fc647-569d-4852-e35a-d261b31f2c63"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL Link: https://www.sec.gov/Archives/edgar/full-index/2019/index.json\n",
            "<Response [403]>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-be47d82629fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mscraping_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-9d53cfd91020>\u001b[0m in \u001b[0;36mscraping_files\u001b[0;34m(year1)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mdecoded_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# the structure is almost identical to other json requests we've made. Go to the item list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_url"
      ],
      "metadata": {
        "id": "R7FTXwQbmKIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel(\"Company.xlsx\")\n",
        "companies_name_list=df['Company'].tolist()\n",
        "company_first=[]\n",
        "for i in companies_name_list:\n",
        "  l1=i.split(' ')\n",
        "  company_first.append(l1[0])\n",
        "form_type=['10-K','8-K','10-Q']"
      ],
      "metadata": {
        "id": "4TAsEyjbT0j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(companies_name_list)"
      ],
      "metadata": {
        "id": "pJlCN1j0m1R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(company_first)"
      ],
      "metadata": {
        "id": "bzeQ5vyWi0Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Master list to store the url of required companies and the element is of the form ( CIK, Company Name, Form Type, Year and Quarter, url)\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "DBEgT1eY2nlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_data = []\n",
        "\n",
        "def parser_idx(url):\n",
        "  file_url = r'%s' % url\n",
        "\n",
        "  # request that new content, this will not be a JSON STRUCTURE!\n",
        "  content = requests.get(file_url,headers=hdr).content\n",
        "\n",
        "# we can always write the content to a file, so we don't need to request it again.\n",
        "  with open('master_20190102.txt', 'wb') as f:\n",
        "       f.write(content)\n",
        "  # let's open it and we will now have a byte stream to play with.\n",
        "  with open('master_20190102.txt','rb') as f:\n",
        "     byte_data = f.read()\n",
        "\n",
        "# Now that we loaded the data, we have a byte stream that needs to be decoded and then split by double spaces.\n",
        "  data = byte_data.decode(\"utf-8\").split('  ')\n",
        "\n",
        "# We need to remove the headers, so look for the end of the header and grab it's index\n",
        "  for index, item in enumerate(data):\n",
        "    if \"ftp://ftp.sec.gov/edgar/\" in item:\n",
        "      start_ind = index\n",
        "\n",
        "# define a new dataset with out the header info.\n",
        "  data_format = data[start_ind + 1:]\n",
        "\n",
        "  \n",
        "\n",
        "# now we need to break the data into sections, this way we can move to the final step of getting each row value.\n",
        "  for index, item in enumerate(data_format):\n",
        "    \n",
        "    # if it's the first index, it won't be even so treat it differently\n",
        "    if index == 0:\n",
        "        clean_item_data = item.replace('\\n','|').split('|')\n",
        "        clean_item_data = clean_item_data[8:]\n",
        "    else:\n",
        "        clean_item_data = item.replace('\\n','|').split('|')\n",
        "        \n",
        "    for index, row in enumerate(clean_item_data):\n",
        "        \n",
        "        # when you find the text file.\n",
        "        if '.txt' in row:\n",
        "\n",
        "            # grab the values that belong to that row. It's 4 values before and one after.\n",
        "            mini_list = clean_item_data[(index - 4): index + 1]\n",
        "            # l1=mini_list[1].split(',')\n",
        "            \n",
        "                        \n",
        "            # if len(mini_list) != 0 and (mini_list[1] in companies_name_list) :\n",
        "            if len(mini_list) != 0 and (mini_list[2] in form_type) :\n",
        "                mini_list[4] = \"https://www.sec.gov/Archives/\" + mini_list[4]\n",
        "                master_data.append(mini_list)\n",
        "                \n",
        "  \n",
        "# grab the first three items\n",
        "  master_data[:3]\n",
        "  "
      ],
      "metadata": {
        "id": "Bp1lfxEF0NoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list_url:\n",
        "  i_length=len(i)\n",
        "  if i[i_length-10:i_length]==\"master.idx\":\n",
        "    try:\n",
        "      parser_idx(i)\n",
        "    except:\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "A99m2dK73gCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Master list to store the data. Each element is in the form (CIK, Company Name, Form Type, Date, URL for file)"
      ],
      "metadata": {
        "id": "j4q91bvsPRqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_dir=[]\n",
        "for i in master_data:\n",
        "  if i[1] in company_first or i[1] in companies_name_list:\n",
        "    master_dir.append(i)\n",
        "  \n",
        "print(master_dir)"
      ],
      "metadata": {
        "id": "gi9T-pOiN67H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(master_dir)\n",
        "final_company=[]\n",
        "for i in master_dir:\n",
        "  if i[1] not in final_company:\n",
        "    final_company.append(i[1])"
      ],
      "metadata": {
        "id": "o4P_rNUgLP-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y-YbS9-AyAEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "qtpvDy1ZM9Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, document in enumerate(master_dir):\n",
        "    \n",
        "    # create a dictionary for each document in the master list\n",
        "    document_dict = {}\n",
        "    document_dict['cik_number'] = document[0]\n",
        "    document_dict['company_name'] = document[1]\n",
        "    document_dict['form_id'] = document[2]\n",
        "    document_dict['date'] = document[3]\n",
        "    document_dict['file_url'] = document[4]\n",
        "    if document[1] in companies_name_list or document[1] in company_first:\n",
        "      master_dir[index] = document_dict\n",
        "    \n"
      ],
      "metadata": {
        "id": "8cOm-gvLMLLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in master_data:\n",
        "  if i['form_id'] not in form_type:\n",
        "    master_data.remove(i)"
      ],
      "metadata": {
        "id": "VSOfDpQpMRFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(master_dir)"
      ],
      "metadata": {
        "id": "Qj52emMCeDZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_dir[0:10]"
      ],
      "metadata": {
        "id": "k4NnkZxtZCRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to parse 10-K\n"
      ],
      "metadata": {
        "id": "-UUXqJjPcE-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraction_10K():\n",
        "  # create the list to hold the statement urls\n",
        "  statements_url = []\n",
        "\n",
        "  for report_dict in master_reports:\n",
        "    \n",
        "    # define the statements we want to look for.\n",
        "    item1 = r\"Consolidated Balance Sheets\"\n",
        "    item2 = r\"Consolidated Statements of Operations and Comprehensive Income (Loss)\"\n",
        "    item3 = r\"Consolidated Statements of Cash Flows\"\n",
        "    item4 = r\"Consolidated Statements of Stockholder's (Deficit) Equity\"\n",
        "    \n",
        "    # store them in a list.\n",
        "    report_list = [item1, item2, item3, item4]\n",
        "    \n",
        "    # if the short name can be found in the report list.\n",
        "    if report_dict['name_short'] in report_list:\n",
        "        \n",
        "        # print some info and store it in the statements url.\n",
        "        print('-'*100)\n",
        "        print(report_dict['name_short'])\n",
        "        print(report_dict['url'])\n",
        "        \n",
        "        statements_url.append(report_dict['url'])"
      ],
      "metadata": {
        "id": "9w9GJsCwbXlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7_SxWSf9chCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}